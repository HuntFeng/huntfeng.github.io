
\chapter{Solving Equations}

Accuracy: How does an algorithm converge to a root?
Robustness: Does the algorithm always converge? or does it diverge?
Efficiency: How fast does an algorithm converge to a root?


There are several ways to compute roots of a given equation $f(x) = 0$: bisection method, newton's method, secant method. 
\section{Intermediate Value Theorem and Bisection}
\subsection*{Intermediate Value Theorem}
\begin{thm}[Intermediate Value Theorem (IVT)]
	Suppose function $f(x)$ is continuous on the interval $[a,b]$. If $f(a)f(b)<0$, then there exists at least one zero of $f(x)$ in $(a,b)$.  
\end{thm}
\begin{proof}
	Assume $f(x)$ is continuous on $[a,b]$ but there is no zeros. Then we must have 
	\[ f(x)>0,\; \forall x\in[a,b]\;\; or \;\; f(x)<0,\; \forall x\in [a,b]  \]
	In either cases, $f(a)f(b) > 0$, we have arrived a contradiction.
\end{proof}
\begin{ex}
	Prove that in $(0,3)$, there exists a solution to the equation 
	\[ x^3 - e^x - 1 = 0 \]
\end{ex}
\begin{proof}
	Since $x^3$, $e^x$, $1$ are all continuous functions on $\mathbb{R}$, so $x^3 - e^x - 1$ is also a continuous function on $\mathbb{R}$.
	
	Moreover, $f(0)f(1) = (0^3 - e^0 - 1)(3^3 - e^3 - 1) < 0$. Thus proved.
\end{proof}

\begin{definition}[Multiplicity of roots]
	Suppose function $f(x)$ is smooth at $x=x_0$(having all orders of derivatives) and $f(x_0)=0$. Then the smallest positive integer $m$ such that 
	\[ f^{(m)}(x_0) \neq 0 \]
	is called the multiplicity of the root. If $m=1$, we call $x_0$ a \textbf{simple root} of $f(x)$; otherwise we call $x_0$ a \textbf{m-th order root} or \textbf{root with multiplicity m} 
\end{definition}

\begin{summary}
	We have to be caution when using IVT:
	\begin{enumerate}
		\item This theorem only applies to continuous functions.
		\item $f(a)f(b) > 0$ DOES NOT mean the function has no zeros in $(a,b)$.
		One classic example is: $f(x) = x^2$ has a root at $x=0$, but $f(-1)f(1) >0$.
		\item This theorem DOES NOT WORK for those roots with multiplicity of even number(just like $f(x) = x^2$). 
	\end{enumerate}
\end{summary}

\subsection*{Bisection}
\begin{thm}
	Suppose $f(x_0) = 0$ and $x_0$ is a root with multiplicity of odd number. Then
	we can apply the following algorithm to approach the solution.
	\begin{enumerate}
		\item Apply IVT to give it a initial interval $(x_l,x_r)$.
		\item Subdivide the interval into half, $(x_l, \frac{x_l+x_r}{2}]$ and $(\frac{x_l + x_r}{2},x_r)$.
		\item Apply IVT to each subinterval to determine the subinterval it is lying in.
		\item Let $x_l$ and $x_r$ be the left and right end points of the subinterval which the $x_0$ is lying in.
		\item Repeat the process until $|x_l - x_0|$ or $|x_r - x_0|$ satisfies the precision requirement. 
	\end{enumerate}
\end{thm}

\begin{ex}
	Find the solution to $x^2 = 2$.
\end{ex}
\begin{solution}
	First we give it an initial interval $(1,2)$.
	
	\begin{tabular}{|c|c|c|}
		\hline 
		Iterations & Subintervals & Choice \\ 
		\hline 
		$0$ &  & $(1,2)$ \\ 
		\hline 
		$1$ & $(1,1.5]$ , $(1.5,2)$  & (1, 1.5)\\
		\hline
		$2$ & $(1,1.25]$, $(1.25,1.5)$ & (1.25, 1.5)\\
		\hline
		$\cdots$ & $\cdots$ & $\cdots$\\
		\hline
	\end{tabular}  
\end{solution}


\section{Newton's Method and Secant Method}
\subsection*{Newton's Method}
\begin{thm}
	Suppose a function $f(x)$ has a root $x^*$ and $f'(x^*) \neq 0$ (simple root), then a way to approach the root is given by a recursive formula
	\[ x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} \]
	and $x_0$ is an initial guess of the function's root. The limit of this sequence $\{x_n\}$ is the root of the equation, that is 
	\[ x^* = \lim_{n\to\infty} x_n \]
\end{thm}

\begin{ex}
	solve the equation $x^2 = 2$. 
\end{ex}
\begin{solution}
	Let $f(x) = x^2-2$. Then apply newton's method with initial guess $x_0=1$. Since
	\[ x_{n+1} = x_n - \frac{x_n^2 -2}{2x_n} \]
	We have
	\begin{align*}
	x_1 &= 1 - \frac{1^2 - 2}{2\times 1} = 1.5\\
	x_2 &= 1.5 - \frac{1.5^2 - 2}{2\times 1.5} = 1.416666667\\
	x_3 &= 1.414215686\\
	\cdots
	\end{align*}
	
	We can continue this process until the value meets the requirement of the precision. 
\end{solution}

\subsection*{Newton's Method for Multi-variable}
\begin{thm}
	Suppose a function $\mathbf{F}(\mathbf{x})$ has a root $\mathbf{x^*}$, then a way to approach the root is given by a recursive formula
	\[ \mathbf{x}_{n+1} = \mathbf{x}_n - \mathbf{J}^{-1}\mathbf{F}(\mathbf{x}_n) \]
	Where $\mathbf{J}$ is Jacobian matrix 
	\[ \mathbf{J} = \begin{bmatrix}
	\frac{\partial f_1}{\partial x_1}(\mathbf{x})& \frac{\partial f_1}{\partial x_2}(\mathbf{x}) & \cdots & \frac{\partial f_1}{\partial x_n}(\mathbf{x})\\
	\frac{\partial f_2}{\partial x_1}(\mathbf{x})& \frac{\partial f_2}{\partial x_2}(\mathbf{x}) & \cdots & \frac{\partial f_2}{\partial x_n}(\mathbf{x})\\
	\vdots & \vdots & \ddots & \vdots\\
	\frac{\partial f_n}{\partial x_1}(\mathbf{x})& \frac{\partial f_n}{\partial x_2}(\mathbf{x}) & \cdots & \frac{\partial f_n}{\partial x_n}(\mathbf{x})\\
	\end{bmatrix} \] 
	and $x_0$ is an initial guess of the function's root. The limit of this sequence $\{x_n\}$ is the root of the equation, that is 
	\[ \mathbf{x}^* = \lim_{n\to\infty} \mathbf{x}_n \]

\end{thm}

\begin{ex}
	For a vector function 
	\[ \mathbf{F}(\mathbf{r}) = \begin{bmatrix}
	r\cos\theta\\ r\sin\theta
	\end{bmatrix} \] 
	
	Find its Jacobian matrix and the inverse Jacobian matrix.
\end{ex}
\begin{solution}
	We see that $f_1(r,\theta) = r\cos\theta$, and $f_2(r,\theta) = r\sin\theta$, so
	\[ \mathbf{J} = \begin{bmatrix}
	\frac{\partial f_1}{\partial r} & \frac{\partial f_1}{\partial \theta}\\
	\frac{\partial f_2}{\partial r} & \frac{\partial f_2}{\partial \theta}
	\end{bmatrix}
	= \begin{bmatrix}
	\cos\theta & -r\sin\theta\\
	\sin\theta & r\cos\theta
	\end{bmatrix} \]
	
	Therefore, its inverse is given by
	\[ \mathbf{J}^{-1} = \frac{1}{r\cos^2\theta + r\sin^2\theta}
	\begin{bmatrix}
	r\cos\theta & r\sin\theta\\
	-\sin\theta & \cos\theta
	\end{bmatrix}
	= \begin{bmatrix}
	\cos\theta & \sin\theta\\
	-\frac{\sin\theta}{r} & \frac{\cos\theta}{r}
	\end{bmatrix}   \]
\end{solution}

Usually, we DO NOT calculate the inverse of the Jacobian matrix, we instead compute a $\mathbf{y}$ such that
\[ \mathbf{J}(\mathbf{x}_n)\mathbf{y}_n = -\mathbf{F}(\mathbf{x}_n) \]
so that $\mathbf{x}_{n+1} = \mathbf{x_n} -\mathbf{J}^{-1}(\mathbf{x_n})\mathbf{F}(\mathbf{x}_n) = \mathbf{x_n+y_n} $. And we can use GE to solve for $\mathbf{y}_n$.

\begin{ex}
	Find the solution to the nonlinear system 
	\[ \mathbf{F}(\mathbf{x}) = \begin{bmatrix}
	3x_1 - cos(x_2x_3) - \frac{1}{2}\\
	x^2 - 81(x_2 + 0.1)^2 +\sin x_3 + 1.06\\
	e^{-x_1x_2} + 20x_3 + \frac{10\pi - 3}{3}
	\end{bmatrix} \]
\end{ex}
\begin{solution}
	First we calculate the Jacobian matrix, 
	\[ \mathbf{J} = \begin{bmatrix}
	3 & x_3\sin x_2x_3 & x_2\sin x_2x_3\\
	2x_1 & -162(x_2+0.1) & \cos x_3\\
	-x_2e^{-x_1x_2} & -x_1e^{-x_1x_2} & 20
	\end{bmatrix} \]
	
	Now we use GE to solve $\mathbf{y}$ such that $\mathbf{J}(\mathbf{x}_0)\mathbf{y} = - \mathbf{F}(\mathbf{x}_0)$. Where
	\[ \mathbf{J}(\mathbf{x}_0) = \begin{bmatrix}
	3 & 9.999833334\times 10^{-4} & 9.999833334\times 10^{-4}\\
	0.2 & -32.4 & 0.9950041653\\
	-0.09900498337 & -0.09900498337 & 20
	\end{bmatrix} \]
	
	and 
	\[ \mathbf{F}(\mathbf{x}_0) = \begin{bmatrix}
	-0.199995\\ -2.269833417\\ 8.462025346
	\end{bmatrix} \]
	
	Thus 
	\[ \mathbf{y}_0 = \begin{bmatrix}
	0.3998696728 \\ -0.08053315147 \\-0.4215204718
	\end{bmatrix} \]
	
	So 
	\[ \mathbf{x}_1 = \mathbf{x}_0 + \mathbf{y}_0 
	=  \begin{bmatrix}
	0.4998696782\\ 0.01946684853\\ -0.5215204718
	\end{bmatrix} \]
	
	Hence, if we continue, we have
	
	\begin{tabular}{|c|c|c|}
		\hline 
		k & $\mathbf{x}_n$ & $\| \mathbf{x}_n - \mathbf{x}_{n-1} \|$ \\ 
		\hline 
		0 & $[0.1000000000, 0.1000000000, -0.1000000000]^T$ &  \\ 
		\hline 
		1 & $[0.4998696728, 0.0194668485, -0.5215204718]^T$ & $0.4215204718$ \\ 
		\hline 
		2 & $[0.5000142403, 0.0015885914, -0.5235569638]^T$ & $1.788\times10^{-2}$ \\ 
		\hline 
		3 & $[0.5000000113, 0.0000124448, -0.5235984500]^T$ & $1.576 \times 10^{-3}$ \\ 
		\hline 
	\end{tabular}  
\end{solution}

\subsection*{Secant Method}
\begin{thm}
	When the explicit derivative of a function $f(x)$ is hard to obtain, we can use the so called secant method to approximate the solution to $f(x) = 0$. 
	\[ x_{n+1} = x_n - f(x_n)\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})} \]
\end{thm}

\begin{ex}
	Solve the equation $x^2 = 2$.
\end{ex}
\begin{solution}
	Let $f(x) = x^2 - 2$, with initial guesses $x_0 = 2$ $x_1 = 1$. Then
	\begin{align*}
	x_{2} &= x_1 - f(x_1)\frac{x_1 - x_{0}}{f(x_1) - f(x_{0})} = 1 - (1^2-2) \frac{1-2}{(1^2-2)- (2^2 - 2)} = 1.3333333\\
	x_3 &= 1.3333333 - (1.3333333^2 - 2)\frac{1.3333333 - 1}{(1.3333333^2 - 2) - (1^2 -2)} = 1.428571435\\
	\cdots 
	\end{align*}
\end{solution}

\section{Order of Convergence}
\begin{definition}
	A method is said to be have an order of convergence $m$ if 
	\[ \lim_{n\to\infty}\frac{|x_{n+1} - x^*|}{|x_n - x^*|^m} = \lambda \]
	Where $\lambda$ is a constant. 
	
	If $n=1$, the method is said to be linear convergence.
	
	If $n=2$, the method is said to be quadratic convergence.
\end{definition}

\begin{thm}
	Newton's method is quadratic convergence.
\end{thm}
\begin{proof}
	Let $g(x_n) = x_n - \frac{f(x)}{f'(x)}$. Then
	\[ g'(x) = \frac{f(x)*f''(x)}{(f'(x))^2} \]
	At $x=x^*$, $g' = 0$ since $f(x^*) = 0$. Thus 
	\[g''(x) = \frac{2f''(x)}{f'(x)} \]
	
	On the other hand, let $\epsilon_{n} \equiv |x_{n} - x^*|$. Then
	\[ \epsilon_{n+1} = \epsilon_{n}g'(x^*) + \frac{g''(x^*)}{2!}\epsilon_{n}^2 + \cdots \]
	
	\[ \lim_{n\to\infty} \frac{|x_{n+1} - x^*|}{|x_n - x^*|^2} = \lim_{n\to\infty}\frac{|\epsilon_{n+1}|}{|\epsilon_{n}|^2} = \frac{g''(x^*)}{2} = const \]
\end{proof}

\begin{summary}
	About the order of convergence
	\begin{enumerate}
		\item Newton's method: Quadratic.
		\item Secant Method: golden ration $\frac{1+\sqrt{5}}{2}$.
		\item  Bisection: Linear.
	\end{enumerate}
\end{summary}
