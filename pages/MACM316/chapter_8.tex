\chapter{Ordinary Differential Equations}
\section{Euler's Method}
\begin{thm}
	Given a first order linear equation and its initial value
	\[ \frac{dy}{dt} = f(t,y),\;\;\;y(t_0) = y_0 \]
	where $f(x,y)$ is a real-valued function. Then Euler's method can be used to compute the numerical values of the unknown function $y$ at points $t_1, t_2, \cdots, t_n$:
		\[w_{k+1} \approx w_{k} + f(t_k, y_k)(t_{k} - t_{k+1}),\;\;k=1,2,\cdots, n-1\]
	These $w_k$ are the approximations of $y_k$.
\end{thm}

\begin{proof}
	Notice that $dy/dt = F(t,y)$ can be written as $dy = F(t,y)dt$. Then it becomes obvious that the difference $\Delta y = y(t+h) - y(t)$ can be estimated by 
	\[ \Delta y \approx F(t,y)\Delta t = F(t,y)h \]
	Thus by rearrange the terms, we have
	\begin{align*}
	y(t+h) \approx y(t) + F(t,y)h
	\end{align*}
\end{proof}

\begin{ex}
	Approximate the solution to the initial-value problem
	\[ y' = y - t^2 + 1, 0 \leq t \leq 2, y(0) = 0.5 \]
	with $h=0.2$. Then find the exact error by comparing to its exact solution $y(t) = (t + 1)^2 - 0.5e^t$. 
	\begin{solution}
		Apply the Euler's method, we have
		\[ w_{i+1} = w_i + h(w_i - t^2_i + 1) = w_i + 0.2[w_i - 0.04i^2 + 1] = 1.2w_i - 0.008i^2 + 0.2,\]
		for $i = 0, 1,... , 9$. So,
		\begin{align*}
		w_1 &= 1.2(0.5) - 0.008(0)^2 + 0.2 = 0.8, \\
		w_2 &= 1.2(0.8) - 0.008(1)^2 + 0.2 = 1.152,
		\end{align*} 
		and so on. The table below shows the comparison between the approximate values at $t_i$ and the actual values.
		\begin{figure}[H]
			\centering
			\includegraphics*[width=8cm]{img/chapter8table0.PNG}
		\end{figure}
	\end{solution}
\end{ex}

\subsection*{Error Analysis of Euler's Method}
\begin{definition} [Lipschitz condition]
	A function $f(t,y)$ is said to satisfy the Lipschitz condition in the variable $y$ on the set $D\subset \mathbb{R}^2$ if there exists a constant $L>0$ such that
	\[ f(t,y_1) - f(t,y_2) \leq L|y_1 - y_2| \]
	where $(t,y_1)$ and $(t,y_2)$ are in $D$. The constant $L$ is called Lipschitz constant.
\end{definition}

\begin{thm}
	Suppose f is continuous and satisfies a Lipschitz condition with constant L on
	\[ D = \{ (t,y)| a\leq t\leq b\;\;\text{and}\;\;-\infty< y < \infty \} \]
	and that a constant M exists with
	\[ |y''(t)|\leq M,\;\forall t \in [a, b], \]
	where $y(t)$ denotes the unique solution to the initial-value problem
	\[ y' = f(t, y), a \leq t \leq b, y(a) = \alpha \]
	Let w0, w1,... , wN be the approximations generated by Euler’s method for some positive integer N. Then, for each $i = 0, 1, 2,... , N$,
	\[ |y(t_i) - w_i| \leq \frac{hM}{2L}[ e^{L(t_i-a)} - 1 ].  \]
\end{thm}

\begin{ex}
	Approximate the solution to the initial-value problem
	\[ y' = y - t^2 + 1, 0 \leq t \leq 2, y(0) = 0.5 \]
	with $h=0.2$. Then find the error bounds to each approximated value.
	
	\begin{solution}
		Because $f(t, y) = y - t^2 + 1$, we have $\partial f/\partial y = 1$ for all $y$, so $L = 1$. For this problem, the exact solution is $y(t) = (t + 1)2 - 0.5e^t$ ,so $y''(t) = 2 - 0.5e^t$ and
		\[ |y''(t)| \leq 0.5e^2 - 2,\;\; \forall t\in [0,2] \]
		
		Using the inequality in the error bound for Euler’s method with $h = 0.2$, $L = 1$, and $M = 0.5e^2 - 2$ gives
		\[ |y_i - w_i| \leq 0.1(0.5e^2 - 2)(e^{t_i} - 1) \]
		
		Hence,
		\begin{align*}
		|y(0.2) - w_1|\leq 0.1(0.5e^2 - 2)(e^{0.2} - 1) = 0.03752,\\
		|y(0.4) - w_2|\leq 0.1(0.5e^2 - 2)(e^{0.4} - 1) = 0.08334,
		\end{align*}
		
		and so on. Table below lists the actual error found in Example 1, together with this error bound. Note that even though the true bound for the second derivative of the solution was used, the error bound is considerably larger than the actual error, especially for increasing values of t.
		
		\begin{figure}[H]
			\centering
			\includegraphics*[width=\linewidth]{img/chapter8table.PNG}
		\end{figure}
	\end{solution}
\end{ex}

\begin{summary}
	Here are somethings you need to know about Euler's Method.
	\begin{enumerate}
		\item 
		EM is robust since $\lim_{h\to 0}|err| = 0$.
		
		\item 
		If $L$ is large, the error will grow exponentially.
		
		\item 
		To be accurate, $h$ must be small, that means we lose efficiency.
		
		\item 
		EM is not practical for most purposes. More accurate ODE methods are essential. 
	\end{enumerate}
\end{summary}

\section{Second Order Runge-Kutta Method}
Suppose that we are going to approximate the solution to the initial value problem
\[  \frac{dy}{dt} = f(t,y),\;\;\;y(t_0) = y_0 \] 
with step size $\Delta t = h$. Denote $w_i$ as the approximations to $y(t_i)$ as usual, and notice that $w_0 = y_0$. We have the following two famous methods.

\subsection*{Modified Euler's Method}
\begin{thm}
	The Modified Euler's Method has the following algorithm:
	\begin{align*}
	w_0 &= y_0\\
	k_1 &= f(t_i, w_i)\\
	k_2 &= f(t_i + h, w_i + hk_1)\\
	w_{i+1} &= w_i + h\frac{k_1+k_2}{2}
	\end{align*}
	
	Where $h = t_{i+1}-t_i$ and the composite error is $O(h^2)$.
\end{thm}

\subsection*{Midpoint Method}
\begin{thm}
	The Midpoint Method has the following algorithm:
	\begin{align*}
	w_0 &= y_0\\
	k_1 &= f(t_i, w_i)\\
	k_2 &= f\left(t_i + \frac{h}{2}, w_i + \frac{h}{2}k_1\right)\\
	w_{i+1} &= w_i + h(k_1+k_2)
	\end{align*}
	
	Where $h = t_{i+1}-t_i$.
\end{thm}

\begin{ex}
	Use the Midpoint method and the Modified Euler method with $N = 10$, $h = 0.2$, $t_i = 0.2i$ , and $w_0 = 0.5$ to approximate the solution to our usual example,
	\[ y' = y - t^2 + 1, 0 \leq t \leq 2, y(0) = 0.5 \]
	
	\begin{solution}
		The difference equations produced from the various formulas are
		
		\[\text{Midpoint method:}\;\; w_{i+1} = 1.22w_i - 0.0088i^2 - 0.008i + 0.218\]
		and 
		\[\text{Modified Euler method:}\;\; w_{i+1} = 1.22w_i - 0.0088i^2 - 0.008i + 0.216 \]
		for each $i=0,1,2,...,9$. The table below shows the values
		
		\begin{figure} [H]
			\centering
			\includegraphics*[width=\linewidth]{img/chapter8table2.PNG}
		\end{figure}
	\end{solution}
\end{ex}


\section{Forth Order Runge-Kutta Method}
This is a famous higher order Runge-Kutta Method, it has a very high accuracy.  
\begin{thm}
	The algorithm of forth order Runge-Kutta method is described as below:
	\begin{align*}
	w_0 &= y_0\\
	k_1 &= f(t_i, w_i)\\
	k_2 &= f\left(t_i + \frac{h}{2}, w_i + \frac{h}{2}k_1\right)\\
	k_3 &= f\left(t_i + \frac{h}{2}, w_i + \frac{h}{2}k_2\right)\\
	k_4 &= f\left(t_i + h, w_i + hk_3\right)\\
	w_{i+1} &= w_i + h\frac{k_1 + 2k_2 + 2k_3 + k_4}{6}
	\end{align*}
	This method has local truncation error $O(h^4)$, provided that the solution $y(t)$ has five continuous derivatives.
\end{thm}

\begin{ex}
	Use the Runge-Kutta method of order four with $h = 0.2$, $N = 10$, and $ti = 0.2i$ to obtain approximations to the solution of the initial-value problem
	\[ y' = y - t^2 + 1, 0 \leq t \leq 2, y(0) = 0.5 \]
	
	\begin{solution}
		The approximation to $y(0.2)$ is obtained by
		\begin{align*}
		w_0 &= 0.5\\
		k_1 &= f(0, 0.5) = 1.5\\
		k_2 &= f\left(0 + \frac{0.2}{2}, 0.5 + \frac{0.2}{2}\times 1.5\right) =1.64\\
		k_3 &= f\left(0 + \frac{0.2}{2}, 0.5 + \frac{0.2}{2}\times 1.64\right) =1.654\\
		k_4 &= f\left(0 + 0.2, 0.5 + 0.2\times 1.654\right) = 1.7908\\
		w_{i+1} &= 0.5 + 0.2\frac{1.5 + 2\times 1.64 + 2\times 1.654 + 1.7908}{6} =0.8292933.
		\end{align*}
		The remaining results and errors are listed in the table below
		\begin{figure} [H]
			\includegraphics*[width=8cm]{img/chapter8table3.PNG}
		\end{figure}
	\end{solution}
\end{ex}

\section{Runge-Kutta-Fehlberg Method}
\begin{thm}
	One popular technique that uses the inequality 
\[ q\leq \left( \frac{\epsilon h}{|\tilde{w}_{i+1} - w_{i+1}|} \right)^{1/n} = \left(\frac{\epsilon}{R}\right)^{1/n} \]
for error control is the Runge-Kutta-Fehlberg method. This technique uses a Runge-Kutta method with local truncation error of order five,
\[ \tilde{w}_{i+1} = w_i + h\left(\frac{16}{135}k_1 + \frac{6656}{12825}k_3 + \frac{28561}{56430}k_4 - \frac{9}{50}k_5 + \frac{2}{55}k_6\right) \]
to estimate the local error in a Runge-Kutta method of order four given by
\[ w_{i+1} = w_i + h\left(\frac{25}{216}k_1 + \frac{1408}{2565}k_3 + \frac{2197}{4104}k_4 - \frac{1}{5}k_5\right)\]
where the coefficient equations are
\begin{align*}
k_1 &= f(t_i,w_i)\\
k_2 &= f\left(t_i + \frac{h}{4}, w_i +\frac{h}{4}k_1\right)\\
k_3 &= f\left(t_i + \frac{3h}{8}, w_i +\frac{3h}{32}k_1 + \frac{9h}{32}k_2\right)\\
k_4 &= f\left(t_i + \frac{12h}{13}, w_i +\frac{1932h}{2197}k_1 - \frac{7200h}{2197}k_2 + \frac{7296h}{2197}k_3\right)\\
k_5 &= f\left(t_i + h, w_i +\frac{439h}{216}k_1 - 8hk_2 + \frac{3680h}{513}k_3 - \frac{845h}{4104}k_4\right)\\
k_6 &= f\left(t_i + \frac{h}{2}, w_i - \frac{8h}{27}k_1 + 2hk_2 - \frac{3544h}{2565}k_3 + \frac{1859h}{4104}k_4 - \frac{11h}{40}k_5 \right)\\
\end{align*}

In error control theory, the value of $q$ determined at the ith step is used for two purposes:\\
\begin{enumerate}
	\item
	When $R>\epsilon$, we reject the initial choice of h at the $i$th step and repeat the calculations using $qh$, and
	\item 
	When $R \leq \epsilon$, we accept the computed value at the $i$th step using the step size $h$ but change the step size to $qh$ for the $(i + 1)$st step.
	Because of the penalty in terms of function evaluations that must be paid if the steps are repeated, $q$ tends to be chosen conservatively. In fact, for the Runge-Kutta-Fehlberg method with $n = 4$, a common choice is
	\[ q = \left( \frac{\epsilon h}{2|\tilde{w}_{i+1} - w_{i+1}|} \right)^{1/4} = 0.84\left( \frac{\epsilon h}{|\tilde{w}_{i+1} - w_{i+1}|} \right)^{1/4} = \left(\frac{\epsilon}{R}\right)^{1/4} \]
\end{enumerate}

\end{thm}
\begin{property}
	An advantage to this method is that only six evaluations of $f$ are required per step. Arbitrary Runge-Kutta methods of orders four and five used together require at least four evaluations of $f$ for the fourth-order method and an additional six for the fifth-order method, for a total of at least 10 function evaluations. So the Runge-Kutta-Fehlberg method has at least a $40\%$ decrease in the number of function evaluations over the use of a pair of arbitrary fourth- and fifth-order methods.
\end{property}

\begin{ex}
	Use the Runge-Kutta-Fehlberg method with a tolerance $TOL = 10^{-5}$, a maximum step size $hmax = 0.25$, and a minimum step size $hmin = 0.01$ to approximate the solution to the initial-value problem
	\[ y' = y - t^2 + 1, 0 \leq t \leq 2, y(0) = 0.5 \]
	and compare the results with the exact solution $y(t) = (t + 1)^2 - 0.5e^t$ .
	
	\begin{solution}
		We will work through the first step of the calculations and then apply the algorithm to determine the remaining results. The initial condition gives $t_0 = 0$ and $w_0 = 0.5$. To determine $w_1$ using $h = 0.25$, the maximum allowable step size, we compute
		\begin{align*}
		k_1 &= f(t_0, w_0) = 1.5\\
		k_2 &= f\left(t_i + \frac{h}{4}, w_i +\frac{h}{4}k_1\right) = 1.5898436\\
		k_3 &= \cdots = 1.6381532\\
		k_4 &= \cdots = 1.8339884\\
		k_5 &= \cdots = 1.8633808\\
		k_6 &= \cdots = 1.6819156
		\end{align*}
		The two approximations to $y(0.25)$ are then found to be
		\begin{align*}
		\tilde{w}_{1} &= w_0 + h\left(\frac{16}{135}k_1 + \frac{6656}{12825}k_3 + \frac{28561}{56430}k_4 - \frac{9}{50}k_5 + \frac{2}{55}k_6\right)\\
		&= 0.5 + \frac{16}{135}0.375 + \frac{6656}{12825}0.4095383 + \frac{28561}{56430}0.4584971 - \frac{9}{50}0.4658452 + \frac{2}{55}0.4204789\\
		&= 0.9204870,
		\end{align*}
		and 
		\begin{align*}
		w_{1} &= w_0 + h\left(\frac{25}{216}k_1 + \frac{1408}{2565}k_3 + \frac{2197}{4104}k_4 - \frac{1}{5}k_5\right)\\
		&= 0.5 + \frac{25}{216}0.375 + \frac{1408}{2565}0.4095383 + \frac{2197}{4104}0.4584971 - \frac{1}{5}0.4658452\\
		&= 0.9204886.
		\end{align*}
		
		This also implies that
		\[R = \frac{|\tilde{w}_1 - w_1|}{h} = 0.00000621388\]
		and
		\[ q = 0.84\left(\frac{\epsilon}{R}\right)^{1/4} =  0.84\left(\frac{0.00001}{0.00000621388}\right)^{1/4} = 0.9461033291 \]
		
		Since $R \leq 10^{-5}$, we can accept the approximation $0.9204886$ for $y(0.25)$, but we should adjust the step size for the next iteration to $h = 0.9461033291(0.25) \approx 0.2365258$. However, only the leading 5 digits of this result would be expected to be accurate because $R$ has only about 5 digits of accuracy. Because we are effectively subtracting the nearly equal numbers $w_i$ and $\tilde{w}_i$ when we compute $R$, there is a good likelihood of round-off error. This is an additional reason for being conservative when computing $q$.
		
		The rest of the results are listed in the table below
		\begin{figure} [H]
			\includegraphics*[width=15cm]{img/chapter8table4.PNG}
		\end{figure}
	\end{solution}
\end{ex}

\section{Multi-Step Method}
\begin{definition}
	An \textbf{m-step multistep method} for solving the initial-value problem
	\[ \frac{dy}{dt} = f(t,y),\;\;a\leq t\leq b,\;\; y(t_0)=y_0 \]
	has a difference equation for finding the approximation $w_{i+1}$ at the mesh point $t_{i+1}$ represented by the following equation, where $m$ is an integer greater than 1:
	\begin{align*}
	w_{i+1} = &a_{m-1}w_i + a_{m-2}w_{i-1} + \cdots + a_{0}w_{i+1-m}\\
	&+ h[b_m f(t_{i+1}, w_{i+1}) + b_{m-1} f(t_i , w_i ) + \cdots + b_0 f(t_{i+1-m}, w_{i+1-m})]
	\end{align*}
	for $i = m-1, m,\cdots , N-1$, where $h = (b-a)/N$, the $a_0, a_1,... , a_{m-1}$ and $b_0, b_1,... , b_m$ are constants, and the starting values
	\[ w_0 = y_0,\;\;w_1 = y_1,\;\; w_2 = y_2,\;\;\cdots\;\;, w_{m-1} = y_{m-1} \]
	are specified.
	
	When $b_m = 0$, the method is called \textbf{explicit}, or \textbf{open}, because
	$w_i+1$ is given explicitly in terms of previously determined values. When $b_m = 0$, the method is called \textbf{implicit}, or \textbf{closed}, because $w_{i+1}$ occurs on both sides of the equation, so $w_{i+1}$ is specified only implicitly.
\end{definition}

\subsection*{Adams-Bashforth Three-Step Explicit Method}
\begin{thm}
	The iteration is given by
\[ w_0 = y_0,\;\; w_1 = y_1,\;\; w_2 = y_2 \]
\[ w_{i+1} = w_i + \frac{h}{12}[23f(t_i, w_i ) - 16f(t_{i-1}, w_{i-1}) + 5f(t_{i-2}, w_{i-2})] \]
where $i = 2, 3,... , N - 1$. The local truncation error is $\tau_i(h) = \frac{3}{8}y^{(4)}(\mu_i)h^3$ for some $\mu_i\in(t_{i-2},t_{i+1})$. 
\end{thm}

\subsection*{Adams-Moulton Three-Step Implicit Method}
Adams-Moulton Implicit methods are derived by using $(t_{i+1}, f(t_{i+1}, y(t_{i+1})))$ as an additional interpolation node in the approximation of the integral
\[ \int_{t_i}^{t_{i+1}} f(t,y(t))dt. \]
Here we only talk about the three-step implicit method.
\begin{thm}
	The iteration is given by
	\[ w_0 = y_0,\;\; w_1 = y_1,\;\; w_2 = y_2 \]
	\[ w_{i+1} = w_i + \frac{h}{24}[9f(t_{i+1}, w_{i+1} ) + 19f(t_i, w_i ) - 5f(t_{i-1}, w_{i-1}) + f(t_{i-2}, w_{i-2})] \]
	where $i = 2, 3,... , N - 1$. The local truncation error is $\tau_i(h) = -\frac{19}{720}y^{(5)}(\mu_i)h^4$ for some $\mu_i\in(t_{i-2},t_{i+1})$. 
\end{thm}

\begin{warning}
	To use this implicit formula explicitly, solve the $w_{i+1}$ first!
\end{warning}


\begin{summary}
	The following table summarize the important aspects of each ODE methods.\\
	
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		Scheme & Use & Global Accuracy & Efficiency & Robustness \\ 
		\hline 
		EM & time-step theory & X & X & X \\ 
		\hline 
		RK2 & fixed $h$ & $h^2$& OK & OK \\ 
		\hline 
		RK4 & fixed $h$ & $h^4$ & OK & OK \\ 
		\hline 
		AB3 & fixed $h$ + costly $\vec{F}$ & $h^3$ & better & OK \\ 
		\hline 
		AM3 & fixed $h$ + costly $\vec{F}$ & $h^4$ & better & OK \\ 
		\hline 
		ode45 & adaptive $h$ + smooth $\vec{y}$ & $h^{5t}$ (best) & best & OK \\ 
		\hline 
		ode23s & adaptive $h$ + smooth $\vec{y}$ & $h^{2t}$ (good)& OK & best \\ 
		\hline 
	\end{tabular} \\

	Various advantages distinguished by ODE problems
	\begin{enumerate}
		\item []
		large problem: fixed $h$, efficient.
		\item []
		small problem: easy to use + robust.
	\end{enumerate}
\end{summary}